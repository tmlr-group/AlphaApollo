trainer:
  nnodes: 1
  n_gpus_per_node: 8

data:
  path: ~/data/rlhf/math/test.parquet
  prompt_key: prompt
  n_samples: 5
  output_path: /opt/tiger/math_Qwen2-7B-Instruct.parquet
  batch_size: 128
  return_raw_chat: True
  train_batch_size: ${data.batch_size}
  val_batch_size: ${data.batch_size}
  max_prompt_length: ${rollout.prompt_length}
  max_response_length: ${rollout.response_length}
  truncation: error
  save2json: False
  json_output_path: ???

model:
  path: ~/models/Qwen2-7B-Instruct
  external_lib: null
rollout:
  name: vllm
  mode: sync # sync: LLM, async: AsyncLLM
  temperature: 1.0
  top_k: -1 # 0 for hf rollout, -1 for vllm rollout
  top_p: 0.7
  prompt_length: 1536
  response_length: 512
  # for vllm rollout
  dtype: bfloat16 # should align with FSDP
  gpu_memory_utilization: 0.5
  ignore_eos: False
  enforce_eager: True
  free_cache_engine: True
  load_format: dummy_dtensor
  tensor_model_parallel_size: 1
  max_num_batched_tokens: 8192
  max_model_len: null
  max_num_seqs: 1024
  log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu
  log_prob_micro_batch_size_per_gpu: 8
  # for fire vllm rollout
  use_fire_sampling: False # enable FIRE https://arxiv.org/abs/2410.21236
  # for hf rollout
  do_sample: True
  disable_log_stats: True
  enable_chunked_prefill: True
  n: 1
actor:
  strategy: fsdp  # This is for backward-compatibility
  ulysses_sequence_parallel_size: 1 # sp size
  fsdp_config:
    fsdp_size: -1

ray_init:
  num_cpus: null # `None` means using all CPUs, which might cause hang if limited in systems like SLURM. Please set to a number allowed then.

env:
  env_name: informal_math_training
  seed: 0
  max_steps: 1
  history_length: 2
  resources_per_worker: # resources for each env worker
    num_cpus: 0.1
    num_gpus: 0
  rollout:
    n: 1 # the group number of envs (for GRPO).
  search:
    log_requests: false
    search_url: "http://127.0.0.1:8000/retrieve" # also support multiple urls: ["http://127.0.0.1:8000/retrieve", "http://127.0.0.1:8001/retrieve"]
    topk: 3
    timeout: 60
  informal_math:
    memory_type: simple
    log_requests: false
    python_code_timeout: 30
    enable_verify: false
    enable_python_code: true # whether to enable python code execution
    enable_local_rag: true # whether to enable local RAG tool
  informal_math_training:
    memory_type: simple
    log_requests: false
    python_code_timeout: 30
    enable_verify: false
    enable_python_code: true # whether to enable python code execution
    enable_local_rag: true # whether to enable local RAG tool
  mol_optim:
    memory_type: simple
    timeout: 5
    use_intermediate_reward: False # whether to use (intermediate rewards + final reward) at each proposal step
    log_requests: False
  sokoban:
    dim_room: [6, 6]
    num_boxes: 1
    search_depth: 30
    mode: tiny_rgb_array
  webshop:
    use_small: True
    human_goals: False

# not used, but need to be defined
algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: True
  use_kl_in_reward: False
  kl_penalty: kl # how to estimate kl divergence
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
    horizon: 10000
    target_kl: 0.1
  use_pf_ppo: False
  pf_ppo:
    reweight_method: pow # ["pow", "max_min", "max_random"]
    weight_pow: 2.0
  filter_groups: # DAPO from https://arxiv.org/abs/2503.14476
    enable: False
    max_num_gen_batches: 10